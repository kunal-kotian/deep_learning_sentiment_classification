{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:35.602352Z",
     "start_time": "2018-06-17T20:12:32.496526Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.894476Z",
     "start_time": "2018-06-17T20:12:35.620047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import contractions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.918257Z",
     "start_time": "2018-06-17T20:12:44.910457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:38:48.953342Z",
     "start_time": "2018-06-17T01:38:48.555941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_feather('../data/reviews_labeled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:08.922482Z",
     "start_time": "2018-06-17T00:37:08.916295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an equal number of positive and negative labeled reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    68001\n",
       "0    68001\n",
       "Name: is_positive, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.is_positive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:12.445017Z",
     "start_time": "2018-06-17T00:37:12.438608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           This is nothing like Chipotle, the food taste ...\n",
       "is_positive                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.iloc[0]        # sample record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:13.727503Z",
     "start_time": "2018-06-17T00:37:13.720729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is nothing like Chipotle, the food taste way better, the quality of the food is great. This is the perfect example of eating in a moms pops restaurant. The atmosphere is awesome, the service is great. If you are looking for good Mexican food this is the place to go, you will not be disappointed. I would go out of my way to eat here that's for sure. I ordered the veggie bowl, since I am vegetarian, best bowl ever. It last me two days.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[0]   # sample review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to break down the text from reviews from its present form, which is a string, to an ordered list of words.  These words are 'features' of each review, and must later be converted into a numerical representation that the neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be considered to be an optional step.  This replaces common word contractions - e.g. `doesn't` is replaced by `does not`.  We use a module called `contractions` (easily installed via `pip install contractions`) that maintains a list of common English contractions and their corresponding expanded versions.\n",
    "\n",
    "Here's a sample usage of the `contractions` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:50.708342Z",
     "start_time": "2018-06-17T00:41:50.703105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  He doesn't know how they've done the job.  I won't allow it.\n",
      "Fixed sentence:     He does not know how they have done the job.  I will not allow it.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He doesn't know how they've done the job.  I won't allow it.\"\n",
    "print('Original sentence: ', sentence)\n",
    "print('Fixed sentence:    ', contractions.fix(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:31.386356Z",
     "start_time": "2018-06-17T00:37:47.559916Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column with text that has contractions expanded\n",
    "reviews_labeled['text_fixed'] = reviews_labeled.text.apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample comparison between an original review and the version with no contractions.  Words like `don't`, `couldn't`, `I've` have been expanded to their full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.310215Z",
     "start_time": "2018-06-17T00:46:01.304898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't normally give five stars unless everything was PERFECT, but I truly couldn't find a single thing to complain about! Service was great, burgers were huge and one of the best I've ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.557774Z",
     "start_time": "2018-06-17T00:46:01.550058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not normally give five stars unless everything was PERFECT, but I truly could not find a single thing to complain about! Service was great, burgers we are huge and one of the best I have ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text_fixed.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:52:01.719141Z",
     "start_time": "2018-06-17T00:52:01.716125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop original text column and rename text_fixed to text\n",
    "reviews_labeled.drop(['text'], axis=1, inplace=True)\n",
    "reviews_labeled.rename(columns={'text_fixed': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_labeled.to_feather('../data/reviews_labeled_no_contractions.feather')\n",
    "reviews_labeled = pd.read_feather('../data/reviews_labeled_no_contractions.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_pickle('../data/reviews_labeled_no_contractions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135997</th>\n",
       "      <td>0</td>\n",
       "      <td>This used to be one of the better places for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135998</th>\n",
       "      <td>0</td>\n",
       "      <td>Ridiculous place. I am shocked this chain stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135999</th>\n",
       "      <td>0</td>\n",
       "      <td>Do not waste your time nor your money here.. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136000</th>\n",
       "      <td>0</td>\n",
       "      <td>Hard to be the king of burgers if you do not h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136001</th>\n",
       "      <td>0</td>\n",
       "      <td>Terrible and Slow Service. Declining Food Qual...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_positive                                               text\n",
       "135997            0  This used to be one of the better places for a...\n",
       "135998            0  Ridiculous place. I am shocked this chain stay...\n",
       "135999            0  Do not waste your time nor your money here.. T...\n",
       "136000            0  Hard to be the king of burgers if you do not h...\n",
       "136001            0  Terrible and Slow Service. Declining Food Qual..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the dataframe\n",
    "reviews_labeled = reviews_labeled.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135997</th>\n",
       "      <td>0</td>\n",
       "      <td>Went here for lunch after driving by several t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135998</th>\n",
       "      <td>1</td>\n",
       "      <td>Delicious!! Absolutely fantastic food. The ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135999</th>\n",
       "      <td>0</td>\n",
       "      <td>Higher prices, smaller portions, less salad ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136000</th>\n",
       "      <td>0</td>\n",
       "      <td>My first visit to this establishment and it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136001</th>\n",
       "      <td>1</td>\n",
       "      <td>Soups and sides are always fresh and delicious...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_positive                                               text\n",
       "135997            0  Went here for lunch after driving by several t...\n",
       "135998            1  Delicious!! Absolutely fantastic food. The ent...\n",
       "135999            0  Higher prices, smaller portions, less salad ch...\n",
       "136000            0  My first visit to this establishment and it is...\n",
       "136001            1  Soups and sides are always fresh and delicious..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(reviews_labeled, '../data/reviews_labeled_no_contractions_shuffled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_pickle('../data/reviews_labeled_no_contractions_shuffled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subset for training\n",
    "reviews_labeled = reviews_labeled.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can perform an `80-20` split of the labeled dataset into training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews_labeled, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108801, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27201, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = train.text.values.tolist()\n",
    "sentiment_train = train.is_positive.values.reshape(len(train), 1)\n",
    "# One-hot encode the target labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "sentiment_train = onehot_encoder.fit_transform(sentiment_train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = test.text.values.tolist()\n",
    "sentiment_test = test.is_positive.values.reshape(len(test), 1)\n",
    "# One-hot encode the target labels\n",
    "sentiment_test = onehot_encoder.transform(sentiment_test)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the built-in tokenizer from Keras to convert each review currently represented as a single text string, into a list of word tokens.\n",
    "\n",
    "This blog provides a good explanation of the process:\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/\n",
    "\n",
    "The crux is that Keras' tokenizer performs a 2-step process:\n",
    "\n",
    "**Step 1**: Split text strings (reviews) into their constituent words. We specify the character to be used for splitting sentences; in this case, it will be the space character.\n",
    "\n",
    "**Step 2**: Take all words split out from the sentences and rank them in the decreasing order of their counts.  So, the most common word will be ranked 1.\n",
    "\n",
    "**Step 3**: Represent each word by its rank found in Step 2.  Here, we move from a string representation of each word to an integer representation.\n",
    "\n",
    "Note that we also specify the maximum number of words we want to include in our vocabulary.  If we ask for our vocabulary size to be `n` words, then only the `n` most common words are included; the rest are removed from each reviw.\n",
    "\n",
    "\n",
    "## Fitting and Transforming using the Tokenizer\n",
    "The Keras tokenizer API performs two common preprocessing steps - lowercasing all words and removing punctuations - when it is fitted onto the training set.  Using the fitted tokenizer, we can convert any new text string into an equivalent list of tokens using the `texts_to_sequences` method.  Such a transformation of new text into tokens only uses the vocabulary known to the tokenizer.  Out-of-vocabulary words are ignored during the transformation of text to tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:**\n",
    "\n",
    "The `Tokenizer` starts assigning ids/indices to each word starting from `1`.  There is no word with id `0`.  This must be kept in mind when using the integer id for any word to index into a tensor/array of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # the maximum size of our vocabulary\n",
    "# import the built-in tokenizer from Keras\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                  lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer on reviews from the training set\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "# convert text into sequences of integer tokens\n",
    "text_train_intseq = tokenizer.texts_to_sequences(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text review: \n",
      " I would love to come here with my kids but it is dangerous specifically after school. I do admit they are trying to keep the rowdy kids out but they are not the police and can hardly keep things safe. I am sitting here and personally am observing tweakers and kids who come in grab a drink from the drink machine without paying. Sad and it ticks me off\n",
      "\n",
      "One of the dirtiest mcdonalds I have ever been in\n",
      "\n",
      "Integer sequence representation: \n",
      " [3, 47, 82, 5, 98, 32, 22, 17, 437, 21, 9, 7, 5242, 1999, 95, 1104, 3, 46, 1994, 19, 13, 397, 5, 428, 1, 6681, 437, 43, 21, 19, 13, 10, 1, 4432, 2, 81, 1420, 428, 329, 2305, 3, 60, 518, 32, 2, 1411, 60, 2, 437, 159, 98, 14, 875, 4, 217, 53, 1, 217, 1507, 354, 821, 718, 2, 9, 52, 141, 49, 11, 1, 8263, 1828, 3, 18, 109, 59, 14]\n"
     ]
    }
   ],
   "source": [
    "# A sample showing the transformation of text_train into text_train_intseq\n",
    "print('Text review: \\n', text_train[0])\n",
    "print('\\nInteger sequence representation: \\n', text_train_intseq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2index` is a dict mapping each word (string) to its unique index (rank) assigned by the tokenizer.  For example, the id/index associated with the word `apple` can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['apple']   # index assigned to the word 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the reviews in the test data using the tokenizer fitted on the training reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test_intseq = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **GloVe** embeddings to represent words in the text.  Using pre-trained embeddings to represent words in a neural network involves the following steps:\n",
    "\n",
    "1. Load the GloVe embeddings file. The file has a word on each line, followed by its embedding vector representation on the same line.\n",
    "2. Only read lines corresponding to words in our pre-selected vocabulary.  In this case, we decided to restrict our vocabulary to the `10,000` most common words; hence, we will only read in word embeddings for words in `word2index` with index <= `10,000`.\n",
    "3. The word embeddings we read in step 2 will be added to an embedding array in the row corresponding to that word's index number found from `word2index`.  (Minor detail: we'll subtract `1` from the index to get the correct row number in the embedding array because integer index assignment in the tokenizer starts from `1`, not `0`).\n",
    "\n",
    "The GloVe embeddings can be downloaded from here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:58:22.677985Z",
     "start_time": "2018-06-17T00:58:12.760915Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 50    # we use 50-dimensional GloVe embeddings\n",
    "emb = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "with open('../data/glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content = line.strip().split(' ')\n",
    "        word = content[0]                                    # string representation\n",
    "        if word in word2index:\n",
    "            index = word2index[word]           # tokenizer index corresponding to word\n",
    "            if index <= vocab_size:\n",
    "                emb_word = np.asarray(content[1:], dtype='float32')  # numerical embedding\n",
    "                # subtract 1 from index because tokenizer indexing started from 1\n",
    "                emb[index - 1, :] = emb_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape   # (number of words in vocabulary) x (embedding size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the length of each review, i.e. the number of tokens in each review.  This step is required for dealing with variable length sequences (since our reviews do not have the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len_train = []\n",
    "for review in text_train_intseq:\n",
    "    review_len_train.append(len(review))\n",
    "    \n",
    "review_len_train = np.array(review_len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_train)    # we have some empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean mask to filter out empty reviews\n",
    "to_keep_ind = [True if review_len_train[i] > 10 else False for i in range(len(review_len_train))]\n",
    "\n",
    "# apply filtering mask to all relevant arrays\n",
    "review_len_train = review_len_train[to_keep_ind]\n",
    "sentiment_train = sentiment_train[to_keep_ind, :]\n",
    "text_train_intseq = np.array(text_train_intseq)[to_keep_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_train)    # we have some empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len_test = []\n",
    "for review in text_test_intseq:\n",
    "    review_len_test.append(len(review))\n",
    "    \n",
    "review_len_test = np.array(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean mask to filter out empty reviews\n",
    "to_keep_ind = [True if review_len_test[i] > 10 else False for i in range(len(review_len_test))]\n",
    "\n",
    "# apply filtering mask to all relevant arrays\n",
    "review_len_test = review_len_test[to_keep_ind]\n",
    "sentiment_test = sentiment_test[to_keep_ind, :]\n",
    "text_test_intseq = np.array(text_test_intseq)[to_keep_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_OUTPUTS = 2      # output, positive or negative label\n",
    "NUM_NEURONS = 24      # number of neurons in each layer\n",
    "NUM_LAYERS = 2       # number of stacked layers of recurrent units\n",
    "DROPOUT_PROB = 0.6   # probability of dropout\n",
    "\n",
    "input_len = len(text_train_intseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size type:.... <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "word_ids_batch:  (?, ?)\n",
      "y_batch:  (?, 2)\n",
      "seq_len_batch:  (?,)\n",
      "X_emb_batch:  (?, ?, 50)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Set up data batching\n",
    "input_sequence = zip(text_train_intseq, sentiment_train, review_len_train)\n",
    "test_sequence = zip(text_test_intseq, sentiment_test, review_len_test)\n",
    "\n",
    "def generator_train():\n",
    "    global input_sequence\n",
    "    while True:\n",
    "        try:\n",
    "            elem = next(input_sequence)\n",
    "            yield tuple((np.array(elem[0]), elem[1], elem[2]))\n",
    "        except StopIteration:\n",
    "            input_sequence = zip(text_train_intseq, sentiment_train, review_len_train)\n",
    "            return   # raises OutOfRangeError which is sent to downstream iterators\n",
    "        \n",
    "def generator_test():\n",
    "    global test_sequence\n",
    "    while True:\n",
    "        try:\n",
    "            elem = next(test_sequence)\n",
    "            yield tuple((np.array(elem[0]), elem[1], elem[2]))\n",
    "        except StopIteration:\n",
    "            test_sequence = zip(text_test_intseq, sentiment_test, review_len_test)\n",
    "            return   # raises OutOfRangeError which is sent to downstream iterators\n",
    "\n",
    "# define types and shapes of objects returned by the generator\n",
    "types=(tf.int32, tf.int32, tf.int32)\n",
    "shapes = (tf.TensorShape([None]), tf.TensorShape([2]), tf.TensorShape(()))\n",
    "\n",
    "# create train and test datasets\n",
    "train_dataset = tf.data.Dataset().from_generator(generator_train, output_types=types)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, padded_shapes=shapes)\n",
    "\n",
    "test_dataset = tf.data.Dataset().from_generator(generator_test, output_types=types)\n",
    "test_dataset = test_dataset.padded_batch(len(text_test_intseq), padded_shapes=shapes)  # here batch is all of test data\n",
    "\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "# Get word_ids (word_ids_batch), labels (y_batch), and sequence length for each batch\n",
    "word_ids_batch, y_batch, seq_len_batch = iter.get_next()\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "batch_size = tf.shape(word_ids_batch)[0]\n",
    "word_ids_batch = tf.reshape(word_ids_batch, [batch_size, -1])\n",
    "y_batch = tf.reshape(y_batch, [batch_size, 2])\n",
    "seq_len_batch = tf.reshape(seq_len_batch, [-1])   # need a flattened list\n",
    "\n",
    "print('batch_size type:....', type(batch_size))\n",
    "print('word_ids_batch: ', word_ids_batch.get_shape())\n",
    "print('y_batch: ', y_batch.get_shape())\n",
    "print('seq_len_batch: ', seq_len_batch.get_shape())\n",
    "\n",
    "# In each batch, take word_ids and look up corresponding word vector X_emb_batch\n",
    "word_embeddings = tf.constant(emb, dtype=tf.float32)\n",
    "X_emb_batch = tf.nn.embedding_lookup(word_embeddings, word_ids_batch)\n",
    "\n",
    "# BUILD MODEL\n",
    "dropout = tf.placeholder_with_default(0.0, shape=())  # allow applying dropout only while training\n",
    "# dropout = 0.3\n",
    "\n",
    "cells = []   # Create a stacked/multi-layered network\n",
    "for _ in range(NUM_LAYERS):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_NEURONS)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=(1.0 - dropout))\n",
    "    cells.append(cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "# Perform dynamic unrolling of the network.  The batched data flows thru this unrolled network.\n",
    "print('X_emb_batch: ', X_emb_batch.get_shape())\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell, X_emb_batch, sequence_length=seq_len_batch, dtype=tf.float32)\n",
    "\n",
    "# Get activation (h) at last 'relevant' time step, relevant means last non-padded step.\n",
    "# seq_len_batch has number of relevant steps (review lengths)\n",
    "# seq_len_batch - 1 will give index of the last relevant element in the 2nd (time step) dimension for every\n",
    "# sequence in the batch\n",
    "last_step_inds = seq_len_batch - tf.constant(1)\n",
    "# create a tensor built from a list of tuples [[seq_1, last_step_1], [seq_2, last_step_2], ...]\n",
    "# using tf.gather_nd, we access the batch size dim using seq_n and the relevant step using last_step_n\n",
    "output_indexer = tf.stack([tf.range(batch_size), last_step_inds], axis=1)\n",
    "outputs_last_step = tf.gather_nd(params=outputs, indices=output_indexer)\n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(outputs_last_step, NUM_OUTPUTS, activation_fn=None)\n",
    "preds_prob = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.losses.softmax_cross_entropy(y_batch, logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Calculate accuracy\n",
    "preds_class = tf.argmax(preds_prob, axis=1, output_type=tf.int32)   # hard predictions\n",
    "labels_class = tf.argmax(y_batch, axis=1, output_type=tf.int32)\n",
    "correct_class = tf.equal(preds_class, labels_class)  # boolean checking equality of predicted and actual classes\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_class, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 0, Loss: 0.4094\n",
      "Test Loss: 0.302622\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 1, Loss: 0.4073\n",
      "Test Loss: 0.211605\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 2, Loss: 0.4074\n",
      "Test Loss: 0.307989\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 3, Loss: 0.4123\n",
      "Test Loss: 0.316940\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 4, Loss: 0.3941\n",
      "Test Loss: 0.297733\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 5, Loss: 0.4017\n",
      "Test Loss: 0.247577\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 6, Loss: 0.3937\n",
      "Test Loss: 0.252391\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 7, Loss: 0.4306\n",
      "Test Loss: 0.247179\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 8, Loss: 0.4162\n",
      "Test Loss: 0.263823\n",
      "Training...\n",
      "batch count:  500\n",
      "batch count:  1000\n",
      "batch count:  1500\n",
      "Iter: 9, Loss: 0.4177\n",
      "Test Loss: 0.220893\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "n_batches = input_len // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    loss_for_plot_train = []\n",
    "    loss_for_plot_test = []\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(train_init_op)\n",
    "        print('Training...')\n",
    "        # Loop over the examples in `iterator`, running `train_op`.\n",
    "        tot_loss = 0\n",
    "        try:\n",
    "            count = 0\n",
    "            while True:\n",
    "                count += 1\n",
    "                _, loss_value = sess.run([train_op, loss], feed_dict={dropout: DROPOUT_PROB})\n",
    "                tot_loss += loss_value\n",
    "                if count % 500 == 0:\n",
    "                    print('batch count: ', count)\n",
    "        except tf.errors.OutOfRangeError:  # Thrown at the end of the epoch\n",
    "            pass\n",
    "        # Perform any per-epoch computations here.\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "        loss_for_plot_train.append(tot_loss / n_batches)\n",
    "        \n",
    "        # initialise iterator with test data\n",
    "        sess.run(test_init_op)\n",
    "        try:\n",
    "            while True:\n",
    "                loss_test = sess.run(loss)\n",
    "                print('Test Loss: {:4f}'.format(loss_test))\n",
    "                loss_for_plot_test.append(loss_test)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "            \n",
    "    # Finally, make predictions from trained model\n",
    "    try:\n",
    "        sess.run(test_init_op)\n",
    "        sentiment_pred_prob, sentiment_pred_class, model_accuracy = sess.run([preds_prob, preds_class, accuracy])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        sess.run(test_init_op)\n",
    "        sentiment_pred_prob, sentiment_pred_class, model_accuracy = sess.run([preds_prob, preds_class, accuracy])\n",
    "    \n",
    "    saver.save(sess, '../saved_models/model3/lstm_classif.model')     # Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(NUM_EPOCHS), loss_for_plot_train, range(NUM_EPOCHS), loss_for_plot_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6434"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in the neural network:\n",
    "np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90911764"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89474267"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assorted Notes\n",
    "1. Effect of punctuations, particularly, ! and ?.  Right now, we've ignored all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions from model\n",
    "# learning rate tune\n",
    "# Lesson - DON'T USE .repeat() if you're looping in an infinite loop over batches\n",
    "# QUESTION: why do we need to pick out last relevant element if we already use sequence_length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = np.arange(27).reshape(3, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8]],\n",
       "\n",
       "       [[ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17]],\n",
       "\n",
       "       [[18, 19, 20],\n",
       "        [21, 22, 23],\n",
       "        [24, 25, 26]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7,  8],\n",
       "       [15, 16, 17],\n",
       "       [24, 25, 26]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 25, 26]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 4, 5]\n",
    "[9, 10, 11]\n",
    "[24, 25, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr_tf = tf.constant(temp_arr)\n",
    "second_dim_addresses = [1, 0, 2]\n",
    "indexer = list(zip(range(3), second_dim_addresses))\n",
    "indices = tf.constant(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_arr_tf: \n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n",
      "gather:\n",
      "[[ 3  4  5]\n",
      " [ 9 10 11]\n",
      " [24 25 26]]\n",
      "-----> indices:  [[0 1]\n",
      " [1 0]\n",
      " [2 2]]\n",
      "-----> indices - 1:  [[-1  0]\n",
      " [ 0 -1]\n",
      " [ 1  1]]\n",
      "shape of temp_arr-tf:\n",
      "3\n",
      "tf.shape(temp_arr_tf):  Tensor(\"Shape_15:0\", shape=(3,), dtype=int32)\n",
      "tf.shape(temp_arr_tf)[0]:  Tensor(\"strided_slice_11:0\", shape=(), dtype=int32)\n",
      "tf.shape(temp_arr_tf)[0].eval():  3\n",
      "tf.range(5).eval():  [0 1 2]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('temp_arr_tf: ')\n",
    "    print(temp_arr_tf.eval())\n",
    "    print('gather:')\n",
    "    print(tf.gather_nd(temp_arr_tf, indices).eval())\n",
    "    print('-----> indices: ', indices.eval())\n",
    "    print('-----> indices - 1: ', (indices - tf.constant(1)).eval())\n",
    "    print('shape of temp_arr-tf:')\n",
    "    print(temp_arr_tf.get_shape().as_list()[0])\n",
    "    print('tf.shape(temp_arr_tf): ', tf.shape(temp_arr_tf))\n",
    "    print('tf.shape(temp_arr_tf)[0]: ', tf.shape(temp_arr_tf)[0])\n",
    "    print('tf.shape(temp_arr_tf)[0].eval(): ', tf.shape(temp_arr_tf)[0].eval())\n",
    "    bsize = tf.shape(temp_arr_tf)[0]\n",
    "    print('tf.range(5).eval(): ', tf.range(bsize).eval())\n",
    "    print(tf.stack([tf.range(bsize), tf.constant([1, 0, 2])], axis=1).eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
