{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:35.602352Z",
     "start_time": "2018-06-17T20:12:32.496526Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.894476Z",
     "start_time": "2018-06-17T20:12:35.620047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kunal/anaconda2/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import contractions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.918257Z",
     "start_time": "2018-06-17T20:12:44.910457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:38:48.953342Z",
     "start_time": "2018-06-17T01:38:48.555941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_feather('../data/reviews_labeled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:08.922482Z",
     "start_time": "2018-06-17T00:37:08.916295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an equal number of positive and negative labeled reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    68001\n",
       "0    68001\n",
       "Name: is_positive, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.is_positive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:12.445017Z",
     "start_time": "2018-06-17T00:37:12.438608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           This is nothing like Chipotle, the food taste ...\n",
       "is_positive                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.iloc[0]        # sample record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:13.727503Z",
     "start_time": "2018-06-17T00:37:13.720729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is nothing like Chipotle, the food taste way better, the quality of the food is great. This is the perfect example of eating in a moms pops restaurant. The atmosphere is awesome, the service is great. If you are looking for good Mexican food this is the place to go, you will not be disappointed. I would go out of my way to eat here that's for sure. I ordered the veggie bowl, since I am vegetarian, best bowl ever. It last me two days.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[0]   # sample review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to break down the text from reviews from its present form, which is a string, to an ordered list of words.  These words are 'features' of each review, and must later be converted into a numerical representation that the neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be considered to be an optional step.  This replaces common word contractions - e.g. `doesn't` is replaced by `does not`.  We use a module called `contractions` (easily installed via `pip install contractions`) that maintains a list of common English contractions and their corresponding expanded versions.\n",
    "\n",
    "Here's a sample usage of the `contractions` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:50.708342Z",
     "start_time": "2018-06-17T00:41:50.703105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  He doesn't know how they've done the job.  I won't allow it.\n",
      "Fixed sentence:     He does not know how they have done the job.  I will not allow it.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He doesn't know how they've done the job.  I won't allow it.\"\n",
    "print('Original sentence: ', sentence)\n",
    "print('Fixed sentence:    ', contractions.fix(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:31.386356Z",
     "start_time": "2018-06-17T00:37:47.559916Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column with text that has contractions expanded\n",
    "reviews_labeled['text_fixed'] = reviews_labeled.text.apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample comparison between an original review and the version with no contractions.  Words like `don't`, `couldn't`, `I've` have been expanded to their full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.310215Z",
     "start_time": "2018-06-17T00:46:01.304898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't normally give five stars unless everything was PERFECT, but I truly couldn't find a single thing to complain about! Service was great, burgers were huge and one of the best I've ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.557774Z",
     "start_time": "2018-06-17T00:46:01.550058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not normally give five stars unless everything was PERFECT, but I truly could not find a single thing to complain about! Service was great, burgers we are huge and one of the best I have ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text_fixed.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:52:01.719141Z",
     "start_time": "2018-06-17T00:52:01.716125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop original text column and rename text_fixed to text\n",
    "reviews_labeled.drop(['text'], axis=1, inplace=True)\n",
    "reviews_labeled.rename(columns={'text_fixed': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews_labeled.to_feather('../data/reviews_labeled_no_contractions.feather')\n",
    "reviews_labeled = pd.read_feather('../data/reviews_labeled_no_contractions.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can perform an `80-20` split of the labeled dataset into training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews_labeled, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108801, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27201, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train = train.text.values.tolist()\n",
    "sentiment_train = train.is_positive.values.reshape(len(train), 1)\n",
    "# One-hot encode the target labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "sentiment_train = onehot_encoder.fit_transform(sentiment_train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = test.text.values.tolist()\n",
    "sentiment_test = test.is_positive.values.reshape(len(test), 1)\n",
    "# One-hot encode the target labels\n",
    "sentiment_test = onehot_encoder.transform(sentiment_test)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the built-in tokenizer from Keras to convert each review currently represented as a single text string, into a list of word tokens.\n",
    "\n",
    "This blog provides a good explanation of the process:\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/\n",
    "\n",
    "The crux is that Keras' tokenizer performs a 2-step process:\n",
    "\n",
    "**Step 1**: Split text strings (reviews) into their constituent words. We specify the character to be used for splitting sentences; in this case, it will be the space character.\n",
    "\n",
    "**Step 2**: Take all words split out from the sentences and rank them in the decreasing order of their counts.  So, the most common word will be ranked 1.\n",
    "\n",
    "**Step 3**: Represent each word by its rank found in Step 2.  Here, we move from a string representation of each word to an integer representation.\n",
    "\n",
    "Note that we also specify the maximum number of words we want to include in our vocabulary.  If we ask for our vocabulary size to be `n` words, then only the `n` most common words are included; the rest are removed from each reviw.\n",
    "\n",
    "\n",
    "## Fitting and Transforming using the Tokenizer\n",
    "The Keras tokenizer API performs two common preprocessing steps - lowercasing all words and removing punctuations - when it is fitted onto the training set.  Using the fitted tokenizer, we can convert any new text string into an equivalent list of tokens using the `texts_to_sequences` method.  Such a transformation of new text into tokens only uses the vocabulary known to the tokenizer.  Out-of-vocabulary words are ignored during the transformation of text to tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:**\n",
    "\n",
    "The `Tokenizer` starts assigning ids/indices to each word starting from `1`.  There is no word with id `0`.  This must be kept in mind when using the integer id for any word to index into a tensor/array of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000    # the maximum size of our vocabulary\n",
    "# import the built-in tokenizer from Keras\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                  lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit tokenizer on reviews from the training set\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "# convert text into sequences of integer tokens\n",
    "text_train_intseq = tokenizer.texts_to_sequences(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text review: \n",
      " They have a good kids menu but my nanny went here today for lunch and found hair in both her meal and in my daughter's food. That is enough for me to decide not to go there. They refunded her money but it grosses me out.  I feel very disappointed as this place is cool and right by the splash pad which makes it very convenient. Hair in one plate is accidental but hair in two plates is just unsanitary. Hair me once shame on you, hair me twice shame on me!\n",
      "\n",
      "Integer sequence representation: \n",
      " [19, 18, 4, 28, 431, 96, 21, 17, 95, 32, 316, 13, 123, 2, 289, 953, 14, 187, 196, 136, 2, 14, 17, 3751, 16, 23, 7, 259, 13, 52, 5, 1433, 10, 5, 48, 36, 19, 5333, 196, 282, 21, 9, 52, 43, 3, 279, 35, 184, 37, 15, 25, 7, 453, 2, 162, 88, 1, 6102, 813, 76, 415, 9, 35, 1387, 953, 14, 50, 373, 7, 21, 953, 14, 140, 682, 7, 40, 4729, 953, 52, 288, 1164, 26, 20, 953, 52, 437, 1164, 26, 52]\n"
     ]
    }
   ],
   "source": [
    "# A sample showing the transformation of text_train into text_train_intseq\n",
    "print('Text review: \\n', text_train[0])\n",
    "print('\\nInteger sequence representation: \\n', text_train_intseq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2index` is a dict mapping each word (string) to its unique index (rank) assigned by the tokenizer.  For example, the id/index associated with the word `apple` can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1316"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['apple']   # index assigned to the word 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the reviews in the test data using the tokenizer fitted on the training reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test_intseq = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **GloVe** embeddings to represent words in the text.  Using pre-trained embeddings to represent words in a neural network involves the following steps:\n",
    "\n",
    "1. Load the GloVe embeddings file. The file has a word on each line, followed by its embedding vector representation on the same line.\n",
    "2. Only read lines corresponding to words in our pre-selected vocabulary.  In this case, we decided to restrict our vocabulary to the `10,000` most common words; hence, we will only read in word embeddings for words in `word2index` with index <= `10,000`.\n",
    "3. The word embeddings we read in step 2 will be added to an embedding array in the row corresponding to that word's index number found from `word2index`.  (Minor detail: we'll subtract `1` from the index to get the correct row number in the embedding array because integer index assignment in the tokenizer starts from `1`, not `0`).\n",
    "\n",
    "The GloVe embeddings can be downloaded from here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:58:22.677985Z",
     "start_time": "2018-06-17T00:58:12.760915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 50    # we use 50-dimensional GloVe embeddings\n",
    "emb = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "with open('../data/glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content = line.strip().split(' ')\n",
    "        word = content[0]                                    # string representation\n",
    "        if word in word2index:\n",
    "            index = word2index[word]           # tokenizer index corresponding to word\n",
    "            if index <= vocab_size:\n",
    "                emb_word = np.asarray(content[1:], dtype='float32')  # numerical embedding\n",
    "                # subtract 1 from index because tokenizer indexing started from 1\n",
    "                emb[index - 1, :] = emb_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape   # (number of words in vocabulary) x (embedding size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the length of each review, i.e. the number of tokens in each review.  This step is required for dealing with variable length sequences (since our reviews do not have the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len = []\n",
    "for review in text_train_intseq:\n",
    "    review_len.append(len(review))\n",
    "    \n",
    "review_len = np.array(review_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108801"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_train_intseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 18,\n",
       " 4,\n",
       " 28,\n",
       " 431,\n",
       " 96,\n",
       " 21,\n",
       " 17,\n",
       " 95,\n",
       " 32,\n",
       " 316,\n",
       " 13,\n",
       " 123,\n",
       " 2,\n",
       " 289,\n",
       " 953,\n",
       " 14,\n",
       " 187,\n",
       " 196,\n",
       " 136,\n",
       " 2,\n",
       " 14,\n",
       " 17,\n",
       " 3751,\n",
       " 16,\n",
       " 23,\n",
       " 7,\n",
       " 259,\n",
       " 13,\n",
       " 52,\n",
       " 5,\n",
       " 1433,\n",
       " 10,\n",
       " 5,\n",
       " 48,\n",
       " 36,\n",
       " 19,\n",
       " 5333,\n",
       " 196,\n",
       " 282,\n",
       " 21,\n",
       " 9,\n",
       " 52,\n",
       " 43,\n",
       " 3,\n",
       " 279,\n",
       " 35,\n",
       " 184,\n",
       " 37,\n",
       " 15,\n",
       " 25,\n",
       " 7,\n",
       " 453,\n",
       " 2,\n",
       " 162,\n",
       " 88,\n",
       " 1,\n",
       " 6102,\n",
       " 813,\n",
       " 76,\n",
       " 415,\n",
       " 9,\n",
       " 35,\n",
       " 1387,\n",
       " 953,\n",
       " 14,\n",
       " 50,\n",
       " 373,\n",
       " 7,\n",
       " 21,\n",
       " 953,\n",
       " 14,\n",
       " 140,\n",
       " 682,\n",
       " 7,\n",
       " 40,\n",
       " 4729,\n",
       " 953,\n",
       " 52,\n",
       " 288,\n",
       " 1164,\n",
       " 26,\n",
       " 20,\n",
       " 953,\n",
       " 52,\n",
       " 437,\n",
       " 1164,\n",
       " 26,\n",
       " 52]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train_intseq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# 1. padding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T01:52:16.439921Z",
     "start_time": "2018-06-18T01:52:16.434835Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_OUTPUTS = 2      # output, positive or negative label\n",
    "NUM_NEURONS = 64      # number of neurons in each layer\n",
    "NUM_LAYERS = 1       # number of stacked layers of recurrent units\n",
    "DROPOUT_PROB = 0.3   # probability of dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T01:52:17.986192Z",
     "start_time": "2018-06-18T01:52:17.950369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "X = tf.placeholder(tf.int32, [BATCH_SIZE, None])      # num of reviews x num of tokens per review (variable)\n",
    "y = tf.placeholder(tf.float32, [BATCH_SIZE, NUM_OUTPUTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = tf.placeholder(tf.int32, [BATCH_SIZE])   # for holding the length of each review\n",
    "\n",
    "# Set up data batching\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "dataset = dataset.shuffle(buffer_size=len(text_train_intseq))     # dataset small enough to hold in memory\n",
    "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=([1, None], [1, 2]))  # batch-level padding\n",
    "dataset = dataset.repeat()\n",
    "iter = dataset.make_initializable_iterator()\n",
    "# Get word_ids and target labels corresponding to X and y provided to dataset for each batch\n",
    "word_ids, targets = iter.get_next()\n",
    "\n",
    "# In each batch, take word_ids and look up corresponding word vector X_emb\n",
    "word_embeddings = tf.constant(emb)\n",
    "X_emb = tf.nn.embedding_lookup(word_embeddings, word_ids)\n",
    "\n",
    "# BUILD MODEL\n",
    "dropout = tf.placeholder_with_default(0.0, shape=())  # allow applying dropout only while training\n",
    "\n",
    "cells = []   # Create a stacked/multi-layered network\n",
    "for _ in range(NUM_LAYERS):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_NEURONS)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=(1.0 - dropout))\n",
    "    cells.append(cell)\n",
    "\n",
    "# Perform dynamic unrolling of the network.  The batched data flows thru this unrolled network.\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell, X_emb, sequence_length=seq_len, dtype=tf.float32)\n",
    "# We're only interested in the last time step; so slice outputs\n",
    "outputs_last_step = outputs[:, -1, :]\n",
    "logits = tf.contrib.layers.fully_connected(outputs_last_step, NUM_OUTPUTS, activation_fn=None)\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.losses.softmax_cross_entropy(targets, logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network\n",
    "n_batches = len(text_train_intseq) // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialize iterator with train data\n",
    "    sess.run(iter.initializer, feed_dict={X: text_train_intseq, y: sentiment_train, batch_size: BATCH_SIZE})\n",
    "    print('Training...')\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer, feed_dict={X: text_test_intseq, y: sentiment_test, batch_size: len(text_test_intseq)})\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T01:32:30.668393Z",
     "start_time": "2018-06-18T01:32:30.663600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000\n",
    "\n",
    "\n",
    "\n",
    "NUM_INPUTS = 1                 # number of channels/features\n",
    "                \n",
    "num_time_steps = len(X_train)  # number of steps to be unrolled at a time during truncated BPTT\n",
    "\n",
    "learning_rate = 0.001 \n",
    "num_epochs = 5000\n",
    "num_layers = 2\n",
    "dropout_prob = 0.3             # probability of dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset = tf.data.Dataset.range(100)\n",
    "# dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaddedBatchDataset shapes: (?, ?), types: tf.int64>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:47:18.995303Z",
     "start_time": "2018-06-17T01:47:18.990263Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 4    # how big we want our vocabulary to be\n",
    "# import the built-in tokenizer from Keras\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                  lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:48:05.784372Z",
     "start_time": "2018-06-17T01:48:05.777636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\", \n",
    "            \"? ? ? ? this ^%&% ^%&% ^%&% ^%&% ^%&% make sense 898 1 1 1 1 1 1\"]\n",
    "tokenizer.fit_on_texts(toytexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:56:54.330875Z",
     "start_time": "2018-06-17T01:56:54.326931Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:57:48.795423Z",
     "start_time": "2018-06-17T01:57:48.785278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(chain.from_iterable([sent.split(' ') for sent in toytexts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:48:05.784372Z",
     "start_time": "2018-06-17T01:48:05.777636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(toytexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:48:05.784372Z",
     "start_time": "2018-06-17T01:48:05.777636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 2, 3], [2], [2, 3], [2, 3], [1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:48:05.784372Z",
     "start_time": "2018-06-17T01:48:05.777636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 1, 'is': 2, 'common': 3, 'the': 4, 'a': 5, 'word': 6, 'so': 7, 'discombobulation': 8, 'not': 9, 'this': 10, 'make': 11, 'sense': 12, '898': 13, 'UNK': 14}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:48:05.784372Z",
     "start_time": "2018-06-17T01:48:05.777636Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T02:05:03.161993Z",
     "start_time": "2018-06-17T02:05:03.158210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = tokenizer.texts_to_sequences([\"this is a new sentence!\", \"Is it though ?\", \"COMMon!!! *&%* WORD\", 'blah', '898'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T02:05:14.732225Z",
     "start_time": "2018-06-17T02:05:14.725593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T02:05:27.842330Z",
     "start_time": "2018-06-17T02:05:27.833665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 14, 14], [2, 14, 14], [3], [14], []]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('is', 5),\n",
       "             ('a', 1),\n",
       "             ('common', 3),\n",
       "             ('word', 1),\n",
       "             ('so', 1),\n",
       "             ('the', 2),\n",
       "             ('discombobulation', 1),\n",
       "             ('not', 1),\n",
       "             ('this', 1),\n",
       "             ('make', 1),\n",
       "             ('sense', 1),\n",
       "             ('898', 1),\n",
       "             ('1', 6)])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = {}\n",
    "with open('../data/glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content = line.strip().split(' ')\n",
    "        word = content[0]\n",
    "        vec = content[1:]\n",
    "        temp[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T22:22:27.434857Z",
     "start_time": "2018-06-16T22:22:27.429303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T22:22:36.889440Z",
     "start_time": "2018-06-16T22:22:36.881193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T02:05:38.242652Z",
     "start_time": "2018-06-17T02:05:38.234773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id[\"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:59:22.158410Z",
     "start_time": "2018-06-17T00:59:22.152662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58544"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id[\"wont\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:59:43.137823Z",
     "start_time": "2018-06-17T00:59:43.120082Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-81368d37324f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Word'"
     ]
    }
   ],
   "source": [
    "word2id[\"Word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T22:27:47.903960Z",
     "start_time": "2018-06-16T22:27:47.128112Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'189087867'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f901b82dda6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'189087867'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '189087867'"
     ]
    }
   ],
   "source": [
    "word2id['189087867']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "word2emb = {}\n",
    "\n",
    "with open('../data/sample.txt', 'r') as f:\n",
    "    for ind, line in enumerate(f):\n",
    "        content = line.strip().split(' ')\n",
    "        word = content[0]\n",
    "        emb = content[1:]\n",
    "        word2id[word] = ind\n",
    "        word2emb[word] = [float(val) for val in emb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"': 8,\n",
       " \"'s\": 9,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'a': 7,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'of': 3,\n",
       " 'the': 0,\n",
       " 'to': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2emb['\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things to try\n",
    "1. Effect of punctuations, particularly, ! and ?.  Right now, we've ignored all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# CONVERT OUTPUT TO list of binary tuples [0,1], [1,0], ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T00:46:19.877450Z",
     "start_time": "2018-06-18T00:46:19.863772Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T00:57:28.426805Z",
     "start_time": "2018-06-18T00:57:28.418358Z"
    }
   },
   "outputs": [],
   "source": [
    "integer_encoded = [0,0,0,1,1,0,1,0,1,1]\n",
    "integer_encoded = np.array(integer_encoded).reshape(len(integer_encoded), 1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_temp = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T00:57:28.858947Z",
     "start_time": "2018-06-18T00:57:28.853243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T00:57:29.661731Z",
     "start_time": "2018-06-18T00:57:29.655123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T00:57:38.530579Z",
     "start_time": "2018-06-18T00:57:38.522150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_array = np.array([1,0,0,0,1]).reshape(5, 1)\n",
    "onehot_encoder.transform(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
