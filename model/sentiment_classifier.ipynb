{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:35.602352Z",
     "start_time": "2018-06-17T20:12:32.496526Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.894476Z",
     "start_time": "2018-06-17T20:12:35.620047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import contractions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.918257Z",
     "start_time": "2018-06-17T20:12:44.910457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:38:48.953342Z",
     "start_time": "2018-06-17T01:38:48.555941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_feather('../data/reviews_labeled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:08.922482Z",
     "start_time": "2018-06-17T00:37:08.916295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an equal number of positive and negative labeled reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    68001\n",
       "0    68001\n",
       "Name: is_positive, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.is_positive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:12.445017Z",
     "start_time": "2018-06-17T00:37:12.438608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           This is nothing like Chipotle, the food taste ...\n",
       "is_positive                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.iloc[0]        # sample record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:13.727503Z",
     "start_time": "2018-06-17T00:37:13.720729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is nothing like Chipotle, the food taste way better, the quality of the food is great. This is the perfect example of eating in a moms pops restaurant. The atmosphere is awesome, the service is great. If you are looking for good Mexican food this is the place to go, you will not be disappointed. I would go out of my way to eat here that's for sure. I ordered the veggie bowl, since I am vegetarian, best bowl ever. It last me two days.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[0]   # sample review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to break down the text from reviews from its present form, which is a string, to an ordered list of words.  These words are 'features' of each review, and must later be converted into a numerical representation that the neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be considered to be an optional step.  This replaces common word contractions - e.g. `doesn't` is replaced by `does not`.  We use a module called `contractions` (easily installed via `pip install contractions`) that maintains a list of common English contractions and their corresponding expanded versions.\n",
    "\n",
    "Here's a sample usage of the `contractions` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:50.708342Z",
     "start_time": "2018-06-17T00:41:50.703105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  He doesn't know how they've done the job.  I won't allow it.\n",
      "Fixed sentence:     He does not know how they have done the job.  I will not allow it.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He doesn't know how they've done the job.  I won't allow it.\"\n",
    "print('Original sentence: ', sentence)\n",
    "print('Fixed sentence:    ', contractions.fix(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:31.386356Z",
     "start_time": "2018-06-17T00:37:47.559916Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column with text that has contractions expanded\n",
    "reviews_labeled['text_fixed'] = reviews_labeled.text.apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample comparison between an original review and the version with no contractions.  Words like `don't`, `couldn't`, `I've` have been expanded to their full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.310215Z",
     "start_time": "2018-06-17T00:46:01.304898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't normally give five stars unless everything was PERFECT, but I truly couldn't find a single thing to complain about! Service was great, burgers were huge and one of the best I've ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.557774Z",
     "start_time": "2018-06-17T00:46:01.550058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not normally give five stars unless everything was PERFECT, but I truly could not find a single thing to complain about! Service was great, burgers we are huge and one of the best I have ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text_fixed.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:52:01.719141Z",
     "start_time": "2018-06-17T00:52:01.716125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop original text column and rename text_fixed to text\n",
    "reviews_labeled.drop(['text'], axis=1, inplace=True)\n",
    "reviews_labeled.rename(columns={'text_fixed': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_labeled.to_feather('../data/reviews_labeled_no_contractions.feather')\n",
    "reviews_labeled = pd.read_feather('../data/reviews_labeled_no_contractions.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_pickle('../data/reviews_labeled_no_contractions.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135997</th>\n",
       "      <td>0</td>\n",
       "      <td>This used to be one of the better places for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135998</th>\n",
       "      <td>0</td>\n",
       "      <td>Ridiculous place. I am shocked this chain stay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135999</th>\n",
       "      <td>0</td>\n",
       "      <td>Do not waste your time nor your money here.. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136000</th>\n",
       "      <td>0</td>\n",
       "      <td>Hard to be the king of burgers if you do not h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136001</th>\n",
       "      <td>0</td>\n",
       "      <td>Terrible and Slow Service. Declining Food Qual...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_positive                                               text\n",
       "135997            0  This used to be one of the better places for a...\n",
       "135998            0  Ridiculous place. I am shocked this chain stay...\n",
       "135999            0  Do not waste your time nor your money here.. T...\n",
       "136000            0  Hard to be the king of burgers if you do not h...\n",
       "136001            0  Terrible and Slow Service. Declining Food Qual..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the dataframe\n",
    "reviews_labeled = reviews_labeled.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_positive</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135997</th>\n",
       "      <td>0</td>\n",
       "      <td>The most disorganized mess of a crew I have se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135998</th>\n",
       "      <td>1</td>\n",
       "      <td>Das ist einer der letzten echten Handwerks-Bäc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135999</th>\n",
       "      <td>0</td>\n",
       "      <td>Apparently they did not season anything the da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136000</th>\n",
       "      <td>0</td>\n",
       "      <td>Food is quite good but this place is way too s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136001</th>\n",
       "      <td>1</td>\n",
       "      <td>Blend of Brazilian, Peruvian and Japanese cuis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_positive                                               text\n",
       "135997            0  The most disorganized mess of a crew I have se...\n",
       "135998            1  Das ist einer der letzten echten Handwerks-Bäc...\n",
       "135999            0  Apparently they did not season anything the da...\n",
       "136000            0  Food is quite good but this place is way too s...\n",
       "136001            1  Blend of Brazilian, Peruvian and Japanese cuis..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(reviews_labeled, '../data/reviews_labeled_no_contractions_shuffled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_pickle('../data/reviews_labeled_no_contractions_shuffled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample subset for training\n",
    "reviews_labeled = reviews_labeled.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can perform an `80-20` split of the labeled dataset into training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews_labeled, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108801, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27201, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = train.text.values.tolist()\n",
    "sentiment_train = train.is_positive.values.reshape(len(train), 1)\n",
    "# One-hot encode the target labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "sentiment_train = onehot_encoder.fit_transform(sentiment_train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = test.text.values.tolist()\n",
    "sentiment_test = test.is_positive.values.reshape(len(test), 1)\n",
    "# One-hot encode the target labels\n",
    "sentiment_test = onehot_encoder.transform(sentiment_test)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the built-in tokenizer from Keras to convert each review currently represented as a single text string, into a list of word tokens.\n",
    "\n",
    "This blog provides a good explanation of the process:\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/\n",
    "\n",
    "The crux is that Keras' tokenizer performs a 2-step process:\n",
    "\n",
    "**Step 1**: Split text strings (reviews) into their constituent words. We specify the character to be used for splitting sentences; in this case, it will be the space character.\n",
    "\n",
    "**Step 2**: Take all words split out from the sentences and rank them in the decreasing order of their counts.  So, the most common word will be ranked 1.\n",
    "\n",
    "**Step 3**: Represent each word by its rank found in Step 2.  Here, we move from a string representation of each word to an integer representation.\n",
    "\n",
    "Note that we also specify the maximum number of words we want to include in our vocabulary.  If we ask for our vocabulary size to be `n` words, then only the `n` most common words are included; the rest are removed from each reviw.\n",
    "\n",
    "\n",
    "## Fitting and Transforming using the Tokenizer\n",
    "The Keras tokenizer API performs two common preprocessing steps - lowercasing all words and removing punctuations - when it is fitted onto the training set.  Using the fitted tokenizer, we can convert any new text string into an equivalent list of tokens using the `texts_to_sequences` method.  Such a transformation of new text into tokens only uses the vocabulary known to the tokenizer.  Out-of-vocabulary words are ignored during the transformation of text to tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:**\n",
    "\n",
    "The `Tokenizer` starts assigning ids/indices to each word starting from `1`.  There is no word with id `0`.  This must be kept in mind when using the integer id for any word to index into a tensor/array of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # the maximum size of our vocabulary\n",
    "# import the built-in tokenizer from Keras\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                  lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer on reviews from the training set\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "# convert text into sequences of integer tokens\n",
    "text_train_intseq = tokenizer.texts_to_sequences(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text review: \n",
      " Yet another great Hawaiian style plate lunch place that reminds me of home! Awesome food and great portions and prices as well. Phone orders available and choke parking as well. Wish it we are a little closer to me but i cannot be to picky haha. Places like this make me less homesick when in search of good comfort food from back home. The last time i went i ordered the Loco Moco, and wow the burgers we are so thick and tender with the mac salad too! Definitely a winner!\n",
      "\n",
      "Integer sequence representation: \n",
      " [470, 201, 33, 1721, 512, 370, 123, 25, 23, 1968, 52, 11, 216, 181, 16, 2, 33, 319, 2, 226, 37, 92, 648, 495, 836, 2, 6116, 635, 37, 92, 432, 9, 8, 12, 4, 134, 1576, 5, 52, 21, 3, 128, 30, 5, 1265, 2280, 224, 42, 15, 122, 52, 419, 54, 14, 2288, 11, 28, 1806, 16, 53, 41, 216, 1, 168, 44, 3, 94, 3, 56, 1, 3662, 5422, 2, 491, 1, 413, 8, 12, 27, 1042, 2, 710, 22, 1, 589, 131, 81, 102, 4, 2690]\n"
     ]
    }
   ],
   "source": [
    "# A sample showing the transformation of text_train into text_train_intseq\n",
    "print('Text review: \\n', text_train[0])\n",
    "print('\\nInteger sequence representation: \\n', text_train_intseq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2index` is a dict mapping each word (string) to its unique index (rank) assigned by the tokenizer.  For example, the id/index associated with the word `apple` can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1351"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['apple']   # index assigned to the word 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the reviews in the test data using the tokenizer fitted on the training reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test_intseq = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **GloVe** embeddings to represent words in the text.  Using pre-trained embeddings to represent words in a neural network involves the following steps:\n",
    "\n",
    "1. Load the GloVe embeddings file. The file has a word on each line, followed by its embedding vector representation on the same line.\n",
    "2. Only read lines corresponding to words in our pre-selected vocabulary.  In this case, we decided to restrict our vocabulary to the `10,000` most common words; hence, we will only read in word embeddings for words in `word2index` with index <= `10,000`.\n",
    "3. The word embeddings we read in step 2 will be added to an embedding array in the row corresponding to that word's index number found from `word2index`.  (Minor detail: we'll subtract `1` from the index to get the correct row number in the embedding array because integer index assignment in the tokenizer starts from `1`, not `0`).\n",
    "\n",
    "The GloVe embeddings can be downloaded from here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:58:22.677985Z",
     "start_time": "2018-06-17T00:58:12.760915Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_dim = 50    # we use 50-dimensional GloVe embeddings\n",
    "emb = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "with open('../data/glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content = line.strip().split(' ')\n",
    "        word = content[0]                                    # string representation\n",
    "        if word in word2index:\n",
    "            index = word2index[word]           # tokenizer index corresponding to word\n",
    "            if index <= vocab_size:\n",
    "                emb_word = np.asarray(content[1:], dtype='float32')  # numerical embedding\n",
    "                # subtract 1 from index because tokenizer indexing started from 1\n",
    "                emb[index - 1, :] = emb_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape   # (number of words in vocabulary) x (embedding size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the length of each review, i.e. the number of tokens in each review.  This step is required for dealing with variable length sequences (since our reviews do not have the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len_train = []\n",
    "for review in text_train_intseq:\n",
    "    review_len_train.append(len(review))\n",
    "    \n",
    "review_len_train = np.array(review_len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_train)    # we have some empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean mask to filter out empty reviews\n",
    "to_keep_ind = [True if review_len_train[i] > 10 else False for i in range(len(review_len_train))]\n",
    "\n",
    "# apply filtering mask to all relevant arrays\n",
    "review_len_train = review_len_train[to_keep_ind]\n",
    "sentiment_train = sentiment_train[to_keep_ind, :]\n",
    "text_train_intseq = np.array(text_train_intseq)[to_keep_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_train)    # we have some empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_len_test = []\n",
    "for review in text_test_intseq:\n",
    "    review_len_test.append(len(review))\n",
    "    \n",
    "review_len_test = np.array(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boolean mask to filter out empty reviews\n",
    "to_keep_ind = [True if review_len_test[i] > 10 else False for i in range(len(review_len_test))]\n",
    "\n",
    "# apply filtering mask to all relevant arrays\n",
    "review_len_test = review_len_test[to_keep_ind]\n",
    "sentiment_test = sentiment_test[to_keep_ind, :]\n",
    "text_test_intseq = np.array(text_test_intseq)[to_keep_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_OUTPUTS = 2      # output, positive or negative label\n",
    "NUM_NEURONS = 16      # number of neurons in each layer\n",
    "NUM_LAYERS = 2       # number of stacked layers of recurrent units\n",
    "DROPOUT_PROB = 0.5   # probability of dropout\n",
    "\n",
    "input_len = len(text_train_intseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size type:.... <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "word_ids_batch:  (?, ?)\n",
      "y_batch:  (?, 2)\n",
      "seq_len_batch:  (?,)\n",
      "X_emb_batch:  (?, ?, 50)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Set up data batching\n",
    "input_sequence = zip(text_train_intseq, sentiment_train, review_len_train)\n",
    "test_sequence = zip(text_test_intseq, sentiment_test, review_len_test)\n",
    "\n",
    "def generator_train():\n",
    "    global input_sequence\n",
    "    while True:\n",
    "        try:\n",
    "            elem = next(input_sequence)\n",
    "            yield tuple((np.array(elem[0]), elem[1], elem[2]))\n",
    "        except StopIteration:\n",
    "            input_sequence = zip(text_train_intseq, sentiment_train, review_len_train)\n",
    "            return   # raises OutOfRangeError which is sent to downstream iterators\n",
    "        \n",
    "def generator_test():\n",
    "    global test_sequence\n",
    "    while True:\n",
    "        try:\n",
    "            elem = next(test_sequence)\n",
    "            yield tuple((np.array(elem[0]), elem[1], elem[2]))\n",
    "        except StopIteration:\n",
    "            test_sequence = zip(text_test_intseq, sentiment_test, review_len_test)\n",
    "            return   # raises OutOfRangeError which is sent to downstream iterators\n",
    "\n",
    "# define types and shapes of objects returned by the generator\n",
    "types=(tf.int32, tf.int32, tf.int32)\n",
    "shapes = (tf.TensorShape([None]), tf.TensorShape([2]), tf.TensorShape(()))\n",
    "\n",
    "# create train and test datasets\n",
    "train_dataset = tf.data.Dataset().from_generator(generator_train, output_types=types)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, padded_shapes=shapes)\n",
    "\n",
    "test_dataset = tf.data.Dataset().from_generator(generator_test, output_types=types)\n",
    "test_dataset = test_dataset.padded_batch(len(text_test_intseq), padded_shapes=shapes)  # here batch is all of test data\n",
    "\n",
    "iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "# Get word_ids (word_ids_batch), labels (y_batch), and sequence length for each batch\n",
    "word_ids_batch, y_batch, seq_len_batch = iter.get_next()\n",
    "\n",
    "# create the initialisation operations\n",
    "train_init_op = iter.make_initializer(train_dataset)\n",
    "test_init_op = iter.make_initializer(test_dataset)\n",
    "\n",
    "batch_size = tf.shape(word_ids_batch)[0]\n",
    "word_ids_batch = tf.reshape(word_ids_batch, [batch_size, -1])\n",
    "y_batch = tf.reshape(y_batch, [batch_size, 2])\n",
    "seq_len_batch = tf.reshape(seq_len_batch, [-1])   # need a flattened list\n",
    "\n",
    "print('batch_size type:....', type(batch_size))\n",
    "print('word_ids_batch: ', word_ids_batch.get_shape())\n",
    "print('y_batch: ', y_batch.get_shape())\n",
    "print('seq_len_batch: ', seq_len_batch.get_shape())\n",
    "\n",
    "# In each batch, take word_ids and look up corresponding word vector X_emb_batch\n",
    "word_embeddings = tf.constant(emb, dtype=tf.float32)\n",
    "X_emb_batch = tf.nn.embedding_lookup(word_embeddings, word_ids_batch)\n",
    "\n",
    "# BUILD MODEL\n",
    "dropout = tf.placeholder_with_default(0.0, shape=())  # allow applying dropout only while training\n",
    "# dropout = 0.3\n",
    "\n",
    "cells = []   # Create a stacked/multi-layered network\n",
    "for _ in range(NUM_LAYERS):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_NEURONS)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=(1.0 - dropout))\n",
    "    cells.append(cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "# Perform dynamic unrolling of the network.  The batched data flows thru this unrolled network.\n",
    "print('X_emb_batch: ', X_emb_batch.get_shape())\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell, X_emb_batch, sequence_length=seq_len_batch, dtype=tf.float32)\n",
    "\n",
    "# Get activation (h) at last 'relevant' time step, relevant means last non-padded step.\n",
    "# seq_len_batch has number of relevant steps (review lengths)\n",
    "# seq_len_batch - 1 will give index of the last relevant element in the 2nd (time step) dimension for every\n",
    "# sequence in the batch\n",
    "last_step_inds = seq_len_batch - tf.constant(1)\n",
    "# create a tensor built from a list of tuples [[seq_1, last_step_1], [seq_2, last_step_2], ...]\n",
    "# using tf.gather_nd, we access the batch size dim using seq_n and the relevant step using last_step_n\n",
    "output_indexer = tf.stack([tf.range(batch_size), last_step_inds], axis=1)\n",
    "outputs_last_step = tf.gather_nd(params=outputs, indices=output_indexer)\n",
    "\n",
    "logits = tf.contrib.layers.fully_connected(outputs_last_step, NUM_OUTPUTS, activation_fn=None)\n",
    "preds_prob = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.losses.softmax_cross_entropy(y_batch, logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Calculate accuracy\n",
    "preds_class = tf.argmax(preds_prob, axis=1, output_type=tf.int32)   # hard predictions\n",
    "labels_class = tf.argmax(y_batch, axis=1, output_type=tf.int32)\n",
    "correct_class = tf.equal(preds_class, labels_class)  # boolean checking equality of predicted and actual classes\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_class, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "batch count:  50\n",
      "batch count:  100\n",
      "batch count:  150\n",
      "batch count:  200\n",
      "batch count:  250\n",
      "batch count:  300\n",
      "batch count:  350\n",
      "batch count:  400\n",
      "batch count:  450\n",
      "batch count:  500\n",
      "batch count:  550\n",
      "batch count:  600\n",
      "batch count:  650\n",
      "batch count:  700\n",
      "batch count:  750\n",
      "batch count:  800\n",
      "batch count:  850\n",
      "batch count:  900\n",
      "batch count:  950\n",
      "batch count:  1000\n",
      "batch count:  1050\n",
      "batch count:  1100\n",
      "batch count:  1150\n",
      "batch count:  1200\n",
      "batch count:  1250\n",
      "batch count:  1300\n",
      "batch count:  1350\n",
      "batch count:  1400\n",
      "batch count:  1450\n",
      "batch count:  1500\n",
      "batch count:  1550\n",
      "batch count:  1600\n",
      "batch count:  1650\n",
      "batch count:  1700\n",
      "Iter: 0, Loss: 0.4134\n",
      "Test Loss: 0.284476\n",
      "Training...\n",
      "batch count:  50\n",
      "batch count:  100\n",
      "batch count:  150\n",
      "batch count:  200\n",
      "batch count:  250\n",
      "batch count:  300\n",
      "batch count:  350\n",
      "batch count:  400\n",
      "batch count:  450\n",
      "batch count:  500\n",
      "batch count:  550\n",
      "batch count:  600\n",
      "batch count:  650\n",
      "batch count:  700\n",
      "batch count:  750\n",
      "batch count:  800\n",
      "batch count:  850\n",
      "batch count:  900\n",
      "batch count:  950\n",
      "batch count:  1000\n",
      "batch count:  1050\n",
      "batch count:  1100\n",
      "batch count:  1150\n",
      "batch count:  1200\n",
      "batch count:  1250\n",
      "batch count:  1300\n",
      "batch count:  1350\n",
      "batch count:  1400\n",
      "batch count:  1450\n",
      "batch count:  1500\n",
      "batch count:  1550\n",
      "batch count:  1600\n",
      "batch count:  1650\n",
      "batch count:  1700\n",
      "Iter: 1, Loss: 0.3992\n",
      "Test Loss: 0.274399\n",
      "Training...\n",
      "batch count:  50\n",
      "batch count:  100\n",
      "batch count:  150\n",
      "batch count:  200\n",
      "batch count:  250\n",
      "batch count:  300\n",
      "batch count:  350\n",
      "batch count:  400\n",
      "batch count:  450\n",
      "batch count:  500\n",
      "batch count:  550\n",
      "batch count:  600\n",
      "batch count:  650\n",
      "batch count:  700\n",
      "batch count:  750\n",
      "batch count:  800\n",
      "batch count:  850\n",
      "batch count:  900\n",
      "batch count:  950\n",
      "batch count:  1000\n",
      "batch count:  1050\n",
      "batch count:  1100\n",
      "batch count:  1150\n",
      "batch count:  1200\n",
      "batch count:  1250\n",
      "batch count:  1300\n",
      "batch count:  1350\n",
      "batch count:  1400\n",
      "batch count:  1450\n",
      "batch count:  1500\n",
      "batch count:  1550\n",
      "batch count:  1600\n",
      "batch count:  1650\n",
      "batch count:  1700\n",
      "Iter: 2, Loss: 0.4000\n",
      "Test Loss: 0.277031\n",
      "Training...\n",
      "batch count:  50\n",
      "batch count:  100\n",
      "batch count:  150\n",
      "batch count:  200\n",
      "batch count:  250\n",
      "batch count:  300\n",
      "batch count:  350\n",
      "batch count:  400\n",
      "batch count:  450\n",
      "batch count:  500\n",
      "batch count:  550\n",
      "batch count:  600\n",
      "batch count:  650\n",
      "batch count:  700\n",
      "batch count:  750\n",
      "batch count:  800\n",
      "batch count:  850\n",
      "batch count:  900\n",
      "batch count:  950\n",
      "batch count:  1000\n",
      "batch count:  1050\n",
      "batch count:  1100\n",
      "batch count:  1150\n",
      "batch count:  1200\n",
      "batch count:  1250\n",
      "batch count:  1300\n",
      "batch count:  1350\n",
      "batch count:  1400\n",
      "batch count:  1450\n",
      "batch count:  1500\n",
      "batch count:  1550\n",
      "batch count:  1600\n",
      "batch count:  1650\n",
      "batch count:  1700\n",
      "Iter: 3, Loss: 0.3931\n",
      "Test Loss: 0.276465\n",
      "Training...\n",
      "batch count:  50\n",
      "batch count:  100\n",
      "batch count:  150\n",
      "batch count:  200\n",
      "batch count:  250\n",
      "batch count:  300\n",
      "batch count:  350\n",
      "batch count:  400\n",
      "batch count:  450\n",
      "batch count:  500\n",
      "batch count:  550\n",
      "batch count:  600\n",
      "batch count:  650\n",
      "batch count:  700\n",
      "batch count:  750\n",
      "batch count:  800\n",
      "batch count:  850\n",
      "batch count:  900\n",
      "batch count:  950\n",
      "batch count:  1000\n",
      "batch count:  1050\n",
      "batch count:  1100\n",
      "batch count:  1150\n",
      "batch count:  1200\n",
      "batch count:  1250\n",
      "batch count:  1300\n",
      "batch count:  1350\n",
      "batch count:  1400\n",
      "batch count:  1450\n",
      "batch count:  1500\n",
      "batch count:  1550\n",
      "batch count:  1600\n",
      "batch count:  1650\n",
      "batch count:  1700\n",
      "Iter: 4, Loss: 0.3967\n",
      "Test Loss: 0.263951\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of lstm_classif_v1.model doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power, beta2_power, fully_connected/biases, fully_connected/biases/Adam, fully_connected/biases/Adam_1, fully_connected/weights, fully_connected/weights/Adam, fully_connected/weights/Adam_1, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adam_1, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power, beta2_power, fully_connected/biases, fully_connected/biases/Adam, fully_connected/biases/Adam_1, fully_connected/weights, fully_connected/weights/Adam, fully_connected/weights/Adam_1, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adam_1, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam_1)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-34-503fb65c7031>\", line 100, in <module>\n    saver = tf.train.Saver()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 832, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 350, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 266, in save_op\n    tensors)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1687, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): ; No such file or directory\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, beta1_power, beta2_power, fully_connected/biases, fully_connected/biases/Adam, fully_connected/biases/Adam_1, fully_connected/weights, fully_connected/weights/Adam, fully_connected/weights/Adam_1, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/Adam_1, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adam, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias/Adam_1, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam, rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam_1)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e1658cb8db3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msentiment_pred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_pred_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm_classif_v1.model'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[1;32m   1719\u001b[0m                   save_path))\n\u001b[0;32m-> 1720\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parent directory of lstm_classif_v1.model doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "n_batches = input_len // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(train_init_op)\n",
    "        print('Training...')\n",
    "        # Loop over the examples in `iterator`, running `train_op`.\n",
    "        tot_loss = 0\n",
    "        try:\n",
    "            count = 0\n",
    "            while True:\n",
    "                count += 1\n",
    "                _, loss_value = sess.run([train_op, loss], feed_dict={dropout: DROPOUT_PROB})\n",
    "                tot_loss += loss_value\n",
    "                if count % 50 == 0:\n",
    "                    print('batch count: ', count)\n",
    "        except tf.errors.OutOfRangeError:  # Thrown at the end of the epoch\n",
    "            pass\n",
    "        # Perform any per-epoch computations here.\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "        \n",
    "        # initialise iterator with test data\n",
    "        try:\n",
    "            sess.run(test_init_op)\n",
    "            print('Test Loss: {:4f}'.format(sess.run(loss)))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            sess.run(test_init_op)\n",
    "            print('Test Loss: {:4f}'.format(sess.run(loss)))\n",
    "            \n",
    "    # Finally, make predictions from trained model\n",
    "    try:\n",
    "        sess.run(test_init_op)\n",
    "        sentiment_pred_prob, sentiment_pred_class, model_accuracy = sess.run([preds_prob, preds_class, accuracy])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        sess.run(test_init_op)\n",
    "        sentiment_pred_prob, sentiment_pred_class, model_accuracy = sess.run([preds_prob, preds_class, accuracy])\n",
    "    \n",
    "    saver.save(sess, './model1/lstm_classif_v1.model')     # Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, './model1/lstm_classif_v1.model')     # Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6434"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in the neural network:\n",
    "np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions from Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89029413"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assorted Notes\n",
    "1. Effect of punctuations, particularly, ! and ?.  Right now, we've ignored all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get predictions from model\n",
    "# learning rate tune\n",
    "# Lesson - DON'T USE .repeat() if you're looping in an infinite loop over batches\n",
    "# QUESTION: why do we need to pick out last relevant element if we already use sequence_length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = np.arange(27).reshape(3, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8]],\n",
       "\n",
       "       [[ 9, 10, 11],\n",
       "        [12, 13, 14],\n",
       "        [15, 16, 17]],\n",
       "\n",
       "       [[18, 19, 20],\n",
       "        [21, 22, 23],\n",
       "        [24, 25, 26]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7,  8],\n",
       "       [15, 16, 17],\n",
       "       [24, 25, 26]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 25, 26]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[3, 4, 5]\n",
    "[9, 10, 11]\n",
    "[24, 25, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr_tf = tf.constant(temp_arr)\n",
    "second_dim_addresses = [1, 0, 2]\n",
    "indexer = list(zip(range(3), second_dim_addresses))\n",
    "indices = tf.constant(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_arr_tf: \n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]]\n",
      "\n",
      " [[ 9 10 11]\n",
      "  [12 13 14]\n",
      "  [15 16 17]]\n",
      "\n",
      " [[18 19 20]\n",
      "  [21 22 23]\n",
      "  [24 25 26]]]\n",
      "gather:\n",
      "[[ 3  4  5]\n",
      " [ 9 10 11]\n",
      " [24 25 26]]\n",
      "-----> indices:  [[0 1]\n",
      " [1 0]\n",
      " [2 2]]\n",
      "-----> indices - 1:  [[-1  0]\n",
      " [ 0 -1]\n",
      " [ 1  1]]\n",
      "shape of temp_arr-tf:\n",
      "3\n",
      "tf.shape(temp_arr_tf):  Tensor(\"Shape_15:0\", shape=(3,), dtype=int32)\n",
      "tf.shape(temp_arr_tf)[0]:  Tensor(\"strided_slice_11:0\", shape=(), dtype=int32)\n",
      "tf.shape(temp_arr_tf)[0].eval():  3\n",
      "tf.range(5).eval():  [0 1 2]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('temp_arr_tf: ')\n",
    "    print(temp_arr_tf.eval())\n",
    "    print('gather:')\n",
    "    print(tf.gather_nd(temp_arr_tf, indices).eval())\n",
    "    print('-----> indices: ', indices.eval())\n",
    "    print('-----> indices - 1: ', (indices - tf.constant(1)).eval())\n",
    "    print('shape of temp_arr-tf:')\n",
    "    print(temp_arr_tf.get_shape().as_list()[0])\n",
    "    print('tf.shape(temp_arr_tf): ', tf.shape(temp_arr_tf))\n",
    "    print('tf.shape(temp_arr_tf)[0]: ', tf.shape(temp_arr_tf)[0])\n",
    "    print('tf.shape(temp_arr_tf)[0].eval(): ', tf.shape(temp_arr_tf)[0].eval())\n",
    "    bsize = tf.shape(temp_arr_tf)[0]\n",
    "    print('tf.range(5).eval(): ', tf.range(bsize).eval())\n",
    "    print(tf.stack([tf.range(bsize), tf.constant([1, 0, 2])], axis=1).eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
