{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:35.602352Z",
     "start_time": "2018-06-17T20:12:32.496526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.894476Z",
     "start_time": "2018-06-17T20:12:35.620047Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import contractions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T20:12:44.918257Z",
     "start_time": "2018-06-17T20:12:44.910457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T01:38:48.953342Z",
     "start_time": "2018-06-17T01:38:48.555941Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_labeled = pd.read_feather('../data/reviews_labeled.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:08.922482Z",
     "start_time": "2018-06-17T00:37:08.916295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136002, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an equal number of positive and negative labeled reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    68001\n",
       "0    68001\n",
       "Name: is_positive, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.is_positive.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:12.445017Z",
     "start_time": "2018-06-17T00:37:12.438608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text           This is nothing like Chipotle, the food taste ...\n",
       "is_positive                                                    1\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.iloc[0]        # sample record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:37:13.727503Z",
     "start_time": "2018-06-17T00:37:13.720729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is nothing like Chipotle, the food taste way better, the quality of the food is great. This is the perfect example of eating in a moms pops restaurant. The atmosphere is awesome, the service is great. If you are looking for good Mexican food this is the place to go, you will not be disappointed. I would go out of my way to eat here that's for sure. I ordered the veggie bowl, since I am vegetarian, best bowl ever. It last me two days.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[0]   # sample review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to break down the text from reviews from its present form, which is a string, to an ordered list of words.  These words are 'features' of each review, and must later be converted into a numerical representation that the neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be considered to be an optional step.  This replaces common word contractions - e.g. `doesn't` is replaced by `does not`.  We use a module called `contractions` (easily installed via `pip install contractions`) that maintains a list of common English contractions and their corresponding expanded versions.\n",
    "\n",
    "Here's a sample usage of the `contractions` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:50.708342Z",
     "start_time": "2018-06-17T00:41:50.703105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  He doesn't know how they've done the job.  I won't allow it.\n",
      "Fixed sentence:     He does not know how they have done the job.  I will not allow it.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"He doesn't know how they've done the job.  I won't allow it.\"\n",
    "print('Original sentence: ', sentence)\n",
    "print('Fixed sentence:    ', contractions.fix(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:41:31.386356Z",
     "start_time": "2018-06-17T00:37:47.559916Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new column with text that has contractions expanded\n",
    "reviews_labeled['text_fixed'] = reviews_labeled.text.apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample comparison between an original review and the version with no contractions.  Words like `don't`, `couldn't`, `I've` have been expanded to their full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.310215Z",
     "start_time": "2018-06-17T00:46:01.304898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't normally give five stars unless everything was PERFECT, but I truly couldn't find a single thing to complain about! Service was great, burgers were huge and one of the best I've ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:46:01.557774Z",
     "start_time": "2018-06-17T00:46:01.550058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not normally give five stars unless everything was PERFECT, but I truly could not find a single thing to complain about! Service was great, burgers we are huge and one of the best I have ever had! $14.95 for a gigantic burger and fries in LV is very affordable! To start, they brought out a big ol' biscuit with honey butter sauce on top that was incredible!! Not long after, our burgers came out. Water was always full, we sat down right away, and it was air conditioned!! Definitely recommend!!\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_labeled.text_fixed.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:52:01.719141Z",
     "start_time": "2018-06-17T00:52:01.716125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop original text column and rename text_fixed to text\n",
    "reviews_labeled.drop(['text'], axis=1, inplace=True)\n",
    "reviews_labeled.rename(columns={'text_fixed': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews_labeled.to_feather('../data/reviews_labeled_no_contractions.feather')\n",
    "reviews_labeled = pd.read_feather('../data/reviews_labeled_no_contractions.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can perform an `80-20` split of the labeled dataset into training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews_labeled, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108801, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27201, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train = train.text.values.tolist()\n",
    "sentiment_train = train.is_positive.values.reshape(len(train), 1)\n",
    "# One-hot encode the target labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "sentiment_train = onehot_encoder.fit_transform(sentiment_train)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_test = test.text.values.tolist()\n",
    "sentiment_test = test.is_positive.values.reshape(len(test), 1)\n",
    "# One-hot encode the target labels\n",
    "sentiment_test = onehot_encoder.transform(sentiment_test)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the built-in tokenizer from Keras to convert each review currently represented as a single text string, into a list of word tokens.\n",
    "\n",
    "This blog provides a good explanation of the process:\n",
    "\n",
    "http://www.developintelligence.com/blog/2017/06/practical-neural-networks-keras-classifying-yelp-reviews/\n",
    "\n",
    "The crux is that Keras' tokenizer performs a 2-step process:\n",
    "\n",
    "**Step 1**: Split text strings (reviews) into their constituent words. We specify the character to be used for splitting sentences; in this case, it will be the space character.\n",
    "\n",
    "**Step 2**: Take all words split out from the sentences and rank them in the decreasing order of their counts.  So, the most common word will be ranked 1.\n",
    "\n",
    "**Step 3**: Represent each word by its rank found in Step 2.  Here, we move from a string representation of each word to an integer representation.\n",
    "\n",
    "Note that we also specify the maximum number of words we want to include in our vocabulary.  If we ask for our vocabulary size to be `n` words, then only the `n` most common words are included; the rest are removed from each reviw.\n",
    "\n",
    "\n",
    "## Fitting and Transforming using the Tokenizer\n",
    "The Keras tokenizer API performs two common preprocessing steps - lowercasing all words and removing punctuations - when it is fitted onto the training set.  Using the fitted tokenizer, we can convert any new text string into an equivalent list of tokens using the `texts_to_sequences` method.  Such a transformation of new text into tokens only uses the vocabulary known to the tokenizer.  Out-of-vocabulary words are ignored during the transformation of text to tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:**\n",
    "\n",
    "The `Tokenizer` starts assigning ids/indices to each word starting from `1`.  There is no word with id `0`.  This must be kept in mind when using the integer id for any word to index into a tensor/array of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000    # the maximum size of our vocabulary\n",
    "# import the built-in tokenizer from Keras\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                                                  lower=True, split=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit tokenizer on reviews from the training set\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "# convert text into sequences of integer tokens\n",
    "text_train_intseq = tokenizer.texts_to_sequences(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text review: \n",
      " Good price and big portion! The interior was nicely decorated as well, although service was a little slow on the Sunday morning we we are there.\n",
      "I had the Street Noodle #1 with the fried chicken, #1 hotness- very flavorful and delicious! The lemongrass ice tea also came with one free re-fill, which the waitress automatically offered without me asking. Will return to try others if I am back again.\n",
      "\n",
      "Cash only!\n",
      "\n",
      "Integer sequence representation: \n",
      " [28, 176, 2, 253, 500, 1, 1319, 6, 1428, 1994, 37, 92, 574, 31, 6, 4, 132, 352, 26, 1, 575, 666, 8, 8, 13, 36, 3, 24, 1, 468, 808, 209, 22, 1, 217, 65, 209, 9736, 34, 590, 2, 78, 1, 3652, 428, 396, 74, 83, 22, 50, 317, 1882, 1359, 76, 1, 213, 3923, 726, 362, 52, 956, 35, 361, 5, 85, 730, 45, 3, 60, 41, 103, 944, 73]\n"
     ]
    }
   ],
   "source": [
    "# A sample showing the transformation of text_train into text_train_intseq\n",
    "print('Text review: \\n', text_train[0])\n",
    "print('\\nInteger sequence representation: \\n', text_train_intseq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2index` is a dict mapping each word (string) to its unique index (rank) assigned by the tokenizer.  For example, the id/index associated with the word `apple` can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1345"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['apple']   # index assigned to the word 'apple'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform the reviews in the test data using the tokenizer fitted on the training reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_test_intseq = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **GloVe** embeddings to represent words in the text.  Using pre-trained embeddings to represent words in a neural network involves the following steps:\n",
    "\n",
    "1. Load the GloVe embeddings file. The file has a word on each line, followed by its embedding vector representation on the same line.\n",
    "2. Only read lines corresponding to words in our pre-selected vocabulary.  In this case, we decided to restrict our vocabulary to the `10,000` most common words; hence, we will only read in word embeddings for words in `word2index` with index <= `10,000`.\n",
    "3. The word embeddings we read in step 2 will be added to an embedding array in the row corresponding to that word's index number found from `word2index`.  (Minor detail: we'll subtract `1` from the index to get the correct row number in the embedding array because integer index assignment in the tokenizer starts from `1`, not `0`).\n",
    "\n",
    "The GloVe embeddings can be downloaded from here:\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T00:58:22.677985Z",
     "start_time": "2018-06-17T00:58:12.760915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 50    # we use 50-dimensional GloVe embeddings\n",
    "emb = np.zeros((vocab_size, emb_dim))\n",
    "\n",
    "with open('../data/glove.6B.50d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content = line.strip().split(' ')\n",
    "        word = content[0]                                    # string representation\n",
    "        if word in word2index:\n",
    "            index = word2index[word]           # tokenizer index corresponding to word\n",
    "            if index <= vocab_size:\n",
    "                emb_word = np.asarray(content[1:], dtype='float32')  # numerical embedding\n",
    "                # subtract 1 from index because tokenizer indexing started from 1\n",
    "                emb[index - 1, :] = emb_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape   # (number of words in vocabulary) x (embedding size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sequence Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate the length of each review, i.e. the number of tokens in each review.  This step is required for dealing with variable length sequences (since our reviews do not have the same length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_len_train = []\n",
    "for review in text_train_intseq:\n",
    "    review_len_train.append(len(review))\n",
    "    \n",
    "review_len_train = np.array(review_len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_len_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_train)    # we have some empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create boolean mask to filter out empty reviews\n",
    "to_keep_ind = [True if review_len_train[i] > 0 else False for i in range(len(review_len_train))]\n",
    "\n",
    "# apply filtering mask to all relevant arrays\n",
    "review_len_train = review_len_train[to_keep_ind]\n",
    "sentiment_train = sentiment_train[to_keep_ind, :]\n",
    "text_train_intseq = np.array(text_train_intseq)[to_keep_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_train)    # we have some empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_len_test = []\n",
    "for review in text_test_intseq:\n",
    "    review_len_test.append(len(review))\n",
    "    \n",
    "review_len_test = np.array(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create boolean mask to filter out empty reviews\n",
    "to_keep_ind = [True if review_len_test[i] > 0 else False for i in range(len(review_len_test))]\n",
    "\n",
    "# apply filtering mask to all relevant arrays\n",
    "review_len_test = review_len_test[to_keep_ind]\n",
    "sentiment_test = sentiment_test[to_keep_ind, :]\n",
    "text_test_intseq = np.array(text_test_intseq)[to_keep_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(review_len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T01:52:16.439921Z",
     "start_time": "2018-06-18T01:52:16.434835Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_OUTPUTS = 2      # output, positive or negative label\n",
    "NUM_NEURONS = 64      # number of neurons in each layer\n",
    "NUM_LAYERS = 1       # number of stacked layers of recurrent units\n",
    "DROPOUT_PROB = 0.3   # probability of dropout\n",
    "\n",
    "input_len = len(text_train_intseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_ids_batch:  (?, ?)\n",
      "y_batch:  (?, 2)\n",
      "seq_len_batch:  (?,)\n",
      "X_emb_batch:  (?, ?, 50)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "# X = tf.placeholder(tf.int32, [None, None])      # num of reviews x num of tokens per review (variable)\n",
    "# y = tf.placeholder(tf.float32, [None, NUM_OUTPUTS])\n",
    "# seq_len = tf.placeholder(tf.int32, [None])   # for holding the length of each review\n",
    "# print('X: ', X.get_shape())\n",
    "\n",
    "# Set up data batching\n",
    "input_sequence = zip(text_train_intseq, sentiment_train, review_len_train)\n",
    "\n",
    "def generator():\n",
    "    while True:\n",
    "        for el in input_sequence:\n",
    "            yield tuple((np.array(el[0]), el[1], el[2]))\n",
    "\n",
    "dataset = tf.data.Dataset().from_generator(generator, output_types=(tf.int32, tf.int32, tf.int32))\n",
    "dataset = dataset.shuffle(buffer_size=input_len)     # dataset small enough to hold in memory\n",
    "# shapes = (tf.TensorShape(None), tf.TensorShape(2), tf.TensorShape(None))\n",
    "# shapes = ([None], [2], [1])\n",
    "# shapes = (tf.TensorShape([None]), tf.TensorShape([2]), tf.TensorShape([1]))\n",
    "shapes = (tf.TensorShape([None]), tf.TensorShape([2]), tf.TensorShape(()))\n",
    "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=shapes)  # batch-level padding\n",
    "dataset = dataset.repeat()\n",
    "iter = dataset.make_initializable_iterator()\n",
    "# Get word_ids (word_ids_batch), labels (y_batch), and sequence length for each batch\n",
    "word_ids_batch, y_batch, seq_len_batch = iter.get_next()\n",
    "\n",
    "\n",
    "batch_size = tf.shape(word_ids_batch)[0]\n",
    "word_ids_batch = tf.reshape(word_ids_batch, [batch_size, -1])\n",
    "y_batch = tf.reshape(y_batch, [batch_size, 2])\n",
    "seq_len_batch = tf.reshape(seq_len_batch, [-1])   # need a flattened list\n",
    "\n",
    "print('word_ids_batch: ', word_ids_batch.get_shape())\n",
    "print('y_batch: ', y_batch.get_shape())\n",
    "print('seq_len_batch: ', seq_len_batch.get_shape())\n",
    "\n",
    "# In each batch, take word_ids and look up corresponding word vector X_emb_batch\n",
    "word_embeddings = tf.constant(emb, dtype=tf.float32)\n",
    "X_emb_batch = tf.nn.embedding_lookup(word_embeddings, word_ids_batch)\n",
    "\n",
    "# BUILD MODEL\n",
    "dropout = tf.placeholder_with_default(0.0, shape=())  # allow applying dropout only while training\n",
    "\n",
    "cells = []   # Create a stacked/multi-layered network\n",
    "for _ in range(NUM_LAYERS):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_NEURONS)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=(1.0 - dropout))\n",
    "    cells.append(cell)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "# Perform dynamic unrolling of the network.  The batched data flows thru this unrolled network.\n",
    "print('X_emb_batch: ', X_emb_batch.get_shape())\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell, X_emb_batch, sequence_length=seq_len_batch, dtype=tf.float32)\n",
    "# We're only interested in the last time step; so slice outputs\n",
    "outputs_last_step = outputs[:, -1, :]\n",
    "logits = tf.contrib.layers.fully_connected(outputs_last_step, NUM_OUTPUTS, activation_fn=None)\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.losses.softmax_cross_entropy(y_batch, logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "n_batches = input_len // BATCH_SIZE\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # initialize iterator with train data\n",
    "    sess.run(iter.initializer)\n",
    "    print('Training...')\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        tot_loss = 0\n",
    "        for _ in range(n_batches):\n",
    "            _, loss_value = sess.run([train_op, loss])\n",
    "            tot_loss += loss_value\n",
    "        print(\"Iter: {}, Loss: {:.4f}\".format(i, tot_loss / n_batches))\n",
    "    # initialise iterator with test data\n",
    "    sess.run(iter.initializer)\n",
    "    print('Test Loss: {:4f}'.format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other things to try\n",
    "1. Effect of punctuations, particularly, ! and ?.  Right now, we've ignored all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to confirm seq_len and that we're getting the last relevant output for loss calc (see Danijar's blog)\n",
    "# look at review len and get rid of very small reviews\n",
    "\n",
    "# what can impact slow training/low core usage?\n",
    "# -- while True?  Try removing it and check\n",
    "# -- shuffling and buffer size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
